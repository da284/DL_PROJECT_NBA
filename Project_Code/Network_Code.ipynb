{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sadna.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/da284/DL_PROJECT_NBA/blob/main/Project_Code/Network_Code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WlEdfKVPRECq"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q16ZOqWFq9qH"
      },
      "source": [
        "#import useful modules\n",
        "\n",
        "import sys\n",
        "import tensorflow as tf\n",
        "print(sys.version)\n",
        "print(tf.__version__)\n",
        "!pip install keras\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from numpy import loadtxt\n",
        "from tensorflow import keras\n",
        "from keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from keras.layers import Dense\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.callbacks import EarlyStopping\n",
        "from sklearn.metrics import accuracy_score\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.layers.convolutional import Conv1D\n",
        "from keras.layers.convolutional import MaxPooling1D\n",
        "from keras.layers.convolutional import AveragePooling1D\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dropout\n",
        "from keras import optimizers\n",
        "from keras.regularizers import l2\n",
        "from keras.regularizers import l1\n",
        "from google.colab import output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xu5xbxDjpDu3"
      },
      "source": [
        "################auxiliary functions################\n",
        "\n",
        "def calc_accuracy(predictions, labels):\n",
        "  '''Calc the accurarcy by the predictions and the real labels'''\n",
        "  win_predictions = [1 if pred > 0.5 else 0 for pred in predictions]#convert predictions into zero one predictions\n",
        "  win_labels = [1 if label > 0.5 else 0 for label in labels]\n",
        "  accuracy = sum([1 if pred == label else 0 for label, pred in zip(win_labels,win_predictions)])/labels.shape[0]\n",
        "  return accuracy\n",
        "\n",
        "def eval_model(model, X, y):\n",
        "  '''Evaluates the keras model on data X with label y'''\n",
        "  predictions = model.predict(X)\n",
        "  return calc_accuracy(predictions, y)\n",
        "  \n",
        "\n",
        "def change_labels_to_binary():\n",
        "  '''Changes the labels to binary, needed for some losses or ML techniques'''\n",
        "  global train_labels, test_labels\n",
        "  train_labels = np.where(train_labels > 0.5, 1, 0)\n",
        "  test_labels = np.where(test_labels > 0.5, 1, 0)\n",
        "\n",
        "def debug(variable):\n",
        "  '''Return debug string of the variable'''\n",
        "  return variable + '=' + repr(eval(variable))\n",
        "\n",
        "def get_model_string():\n",
        "  '''Returns string that represents the current model'''\n",
        "  string = \"\"\n",
        "  string += '------------current model------------\\n'\n",
        "  string += debug('CNN') + '\\n'\n",
        "  if CNN:\n",
        "    for var in ['TRAINING_FILE_NAME','FEATURES_LEN','POOLING','REGULARIZATION','REGULARIZATION_CONST','DROPOUT','BATCH_NORMALIZATION', 'EPOCHS','SGD_EPOCHS','BATCH_SIZE','FILTERS1','FILTERS2', 'CNN_DENSE1', 'CNN_DENSE2', 'VALIDATION_SPLIT', 'OPTIMIZER','LOSS_FN']:\n",
        "      string += debug(var) + '\\n'\n",
        "  else:\n",
        "    for var in ['TRAINING_FILE_NAME','FEATURES_LEN','REGULARIZATION','REGULARIZATION_CONST','DROPOUT','BATCH_NORMALIZATION', 'EPOCHS','SGD_EPOCHS','BATCH_SIZE','DENSE1','DENSE2','DENSE3', 'DENSE4', 'DENSE5', 'VALIDATION_SPLIT', 'OPTIMIZER','LOSS_FN']:\n",
        "      string += debug(var) + '\\n'\n",
        "  string += '-------------------------------------\\n'\n",
        "  return string\n",
        "\n",
        "def zero_one_accuracy(y_true, y_pred):\n",
        "  '''Calculates zero one accuracy through the training of the model'''\n",
        "  y1 = tf.where(y_true > 0.5, 1., 0.)\n",
        "  y2 = tf.where(y_pred > 0.5, 1., 0.)\n",
        "  accuracy = tf.where(y1-y2 == 0, 1., 0.)\n",
        "  return accuracy\n",
        "\n",
        "def certainty_calculator(model, X, labels, down_threshold, up_threshold):\n",
        "  '''Calculates the model's guess certainty for some label between the down and up thresholds (distance from 0.5)'''\n",
        "  #threshold is between 0 and 0.5\n",
        "  if up_threshold > 0.5 or down_threshold < 0 or down_threshold > up_threshold: \n",
        "    raise Exception( \"Threshold is problematic\" )\n",
        "  predictions = model.predict(X)\n",
        "  counter = 0\n",
        "  sum = 0\n",
        "  for l, p in zip(labels, predictions):\n",
        "    if (p > 0.5 + down_threshold and p < 0.5 + up_threshold) or (p < 0.5 - down_threshold and p > 0.5 - up_threshold):#check if the prediction is between the down and up thresholds\n",
        "      if (l > 0.5 and p > 0.5) or (l <= 0.5 and p <= 0.5):#checks if the model guessed right on the data vector\n",
        "        sum += 1\n",
        "      counter += 1\n",
        "  if counter == 0:\n",
        "    print(\"Counter is zero\")#no games in this threshold\n",
        "    return\n",
        "  print(\"CERTAINTY CHECK FOR TRESHOLD BETWEEN: {0} AND {1},PERCENTAGE: {2}, CERTAINTY: {3}\".format(down_threshold,up_threshold,counter/len(predictions),sum/counter))#print the precentage of the games that are between the threshold values, \n",
        "  #and the certainty of guessing right in this thresholds\n",
        "\n",
        "\n",
        "#######################################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uLeau3gWAFAF"
      },
      "source": [
        "########################IMPORTANT FUNCTIONS###############################\n",
        "\n",
        "NUM_OF_GAMES_BACKWARD = 8\n",
        "NUM_OF_FEATURES_PER_GAME = 8\n",
        "TRAINING_FILE_NAME = 'team_training_' + str(NUM_OF_GAMES_BACKWARD) + '_' + str(NUM_OF_FEATURES_PER_GAME) + '_25_2004.csv'#for team data\n",
        "TEST_FILE_NAME = 'team_test_' + str(NUM_OF_GAMES_BACKWARD) + '_' + str(NUM_OF_FEATURES_PER_GAME) + '_25_2004.csv'#for team data\n",
        "\n",
        "NUM_OF_PLAYER_PER_TEAM = 9\n",
        "NUM_OF_FEATURES_PER_PLAYER = 13\n",
        "# TRAINING_FILE_NAME = 'player_training_' + str(NUM_OF_PLAYER_PER_TEAM)  + '_' + str(NUM_OF_GAMES_BACKWARD) + '_25.csv'#for player data\n",
        "# TEST_FILE_NAME = 'player_test_' + str(NUM_OF_PLAYER_PER_TEAM)  + '_' + str(NUM_OF_GAMES_BACKWARD) + '_25.csv'#for player data\n",
        "\n",
        "\n",
        "VERBOSE = 1\n",
        "FEATURES_LEN = None #calculated in get_dataset() \n",
        "VALIDATION_SPLIT = 0.1\n",
        "\n",
        "#####PARAMETERS OF THE MODEL######\n",
        "CNN = False\n",
        "OPTIMIZER = 'adam'\n",
        "LOSS_FN = 'mean_squared_error'#'binary_cross_entropy'#'mae'\n",
        "DROPOUT = None\n",
        "BATCH_NORMALIZATION = False\n",
        "REGULARIZATION_CONST = 0\n",
        "REGULARIZATION = l2(REGULARIZATION_CONST)\n",
        "###CNN###\n",
        "POOLING = 'average'\n",
        "FILTERS1 = 7\n",
        "FILTERS2 = None\n",
        "CNN_DENSE1 = 50\n",
        "CNN_DENSE2 = 50\n",
        "###DENSE###\n",
        "DENSE1 = 75\n",
        "DENSE2 = 75\n",
        "DENSE3 = 75\n",
        "DENSE4 = None\n",
        "DENSE5 = None\n",
        "\n",
        "################\n",
        "\n",
        "EPOCHS = 10\n",
        "SGD_EPOCHS = 10\n",
        "BATCHES_PER_EPOCH = None #calculated in get_dataset() \n",
        "BATCH_SIZE = 32\n",
        "\n",
        "train_samples = None\n",
        "train_labels = None\n",
        "test_samples = None\n",
        "test_labels = None\n",
        "MODELS = [] #saves models result in the grid search\n",
        "\n",
        "def build_model():\n",
        "  '''Builds model by the global network parameters'''\n",
        "  if CNN:\n",
        "    model = Sequential()\n",
        "    if FILTERS1:\n",
        "      model.add(Conv1D(filters=FILTERS1, kernel_size=NUM_OF_FEATURES_PER_PLAYER, strides = NUM_OF_FEATURES_PER_PLAYER, activation='relu', input_shape=(FEATURES_LEN,1), kernel_regularizer=REGULARIZATION)) #for players data\n",
        "      # model.add(Conv1D(filters=FILTERS1, kernel_size=NUM_OF_GAMES_BACKWARD, strides = NUM_OF_GAMES_BACKWARD, activation='relu', input_shape=(FEATURES_LEN,1), kernel_regularizer=REGULARIZATION)) #for teams data\n",
        "    if BATCH_NORMALIZATION:\n",
        "      model.add(BatchNormalization())\n",
        "    if FILTERS2:\n",
        "      if DROPOUT:\n",
        "        model.add(Dropout(DROPOUT))\n",
        "      # model.add(Conv1D(filters=FILTERS2, kernel_size=NUM_OF_FEATURES_PER_GAME, strides = NUM_OF_FEATURES_PER_GAME, activation='relu', kernel_regularizer=REGULARIZATION)) #for team data\n",
        "      model.add(Conv1D(filters=FILTERS2, kernel_size=NUM_OF_PLAYER_PER_TEAM, strides = NUM_OF_PLAYER_PER_TEAM, activation='relu', kernel_regularizer=REGULARIZATION)) #for player data\n",
        "    if DROPOUT:\n",
        "      model.add(Dropout(DROPOUT))\n",
        "    if POOLING:\n",
        "      if POOLING == 'max':\n",
        "        model.add(MaxPooling1D(pool_size=NUM_OF_PLAYER_PER_TEAM))\n",
        "      elif POOLING == 'average':\n",
        "        model.add(AveragePooling1D(pool_size=NUM_OF_PLAYER_PER_TEAM))\n",
        "      else:\n",
        "        raise ValueError('Mistake in the pooling variable')\n",
        "    model.add(Flatten())\n",
        "    if CNN_DENSE1:\n",
        "      model.add(Dense(CNN_DENSE1, activation='relu', kernel_regularizer=REGULARIZATION))\n",
        "    if CNN_DENSE2:\n",
        "      model.add(Dense(CNN_DENSE1, activation='relu', kernel_regularizer=REGULARIZATION))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "  else: #builds fully connected network\n",
        "    model = Sequential()\n",
        "    model.add(Dense(DENSE1, input_dim=FEATURES_LEN, activation='relu', kernel_regularizer=REGULARIZATION))\n",
        "    if DENSE2:\n",
        "      model.add(Dense(DENSE2, activation='relu', kernel_regularizer=REGULARIZATION))\n",
        "      if DROPOUT:\n",
        "        model.add(Dropout(DROPOUT))\n",
        "      if BATCH_NORMALIZATION:\n",
        "        model.add(BatchNormalization())\n",
        "    if DENSE3:\n",
        "      model.add(Dense(DENSE3, activation='relu', kernel_regularizer=REGULARIZATION))\n",
        "    if DENSE4:\n",
        "      model.add(Dense(DENSE4, activation='relu', kernel_regularizer=REGULARIZATION))\n",
        "    if DENSE5:\n",
        "      model.add(Dense(DENSE5, activation='relu', kernel_regularizer=REGULARIZATION))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "  optimizer = OPTIMIZER\n",
        "  if optimizer == 'sgd':\n",
        "    learning_rate = 0.1\n",
        "    decay_rate = 1e-6 #learning_rate / EPOCHS\n",
        "    momentum = 0.8\n",
        "    optimizer = SGD(learning_rate=learning_rate, momentum=momentum, decay=decay_rate, nesterov=False)\n",
        "  loss_fn = LOSS_FN\n",
        "  if LOSS_FN == 'mae':\n",
        "    loss_fn = tf.keras.losses.MeanAbsoluteError()\n",
        "  if LOSS_FN == 'binary_cross_entropy':\n",
        "    loss_fn = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
        "    change_labels_to_binary() #need to change labels for binary cross entropy loss\n",
        "  model.compile(loss=loss_fn, optimizer=optimizer, metrics=[zero_one_accuracy])#'mse', 'mae' #compile the model\n",
        "  return model\n",
        "  \n",
        "\n",
        "def get_dataset():\n",
        "  '''Get the dataset from the respective files, if CNN is true, changes the shape of the samples'''\n",
        "  global train_samples, train_labels, test_samples, test_labels, FEATURES_LEN, BATCHES_PER_EPOCH\n",
        "  # load the training set\n",
        "  dataset = np.asarray(pd.read_csv('drive/MyDrive/' + TRAINING_FILE_NAME, delimiter=',').values.tolist())\n",
        "  np.random.shuffle(dataset)\n",
        "  train_samples = dataset[:,0:-1]\n",
        "  train_labels = dataset[:,-1]\n",
        "  FEATURES_LEN = train_samples.shape[1] #get the number of features from dataset\n",
        "  if not BATCHES_PER_EPOCH:\n",
        "    BATCHES_PER_EPOCH = int((train_samples.shape[0]*(1-VALIDATION_SPLIT)/BATCH_SIZE))#calculate how many batches there are for one epoch\n",
        "  # load the test set\n",
        "  dataset = np.asarray(pd.read_csv('drive/MyDrive/' + TEST_FILE_NAME, delimiter=',').values.tolist())\n",
        "  test_samples = dataset[:,0:-1]\n",
        "  test_labels = dataset[:,-1]\n",
        "  if CNN:\n",
        "    train_samples = train_samples.reshape((len(train_samples),FEATURES_LEN,1))\n",
        "    test_samples = test_samples.reshape((len(test_samples),FEATURES_LEN,1))\n",
        "\n",
        "def print_history(history):\n",
        "  '''Prints the history of the model, returns maximum of the validation'''\n",
        "  print('\\n\\nHISTORY:')\n",
        "  #prints the loss graph\n",
        "  plt.plot(history.history['loss'])\n",
        "  plt.plot(history.history['val_loss'])\n",
        "  plt.title('model loss')\n",
        "  plt.ylabel(LOSS_FN)\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['train', 'validation'], loc='upper left')\n",
        "  plt.show()\n",
        "  print()\n",
        "  # prints the accuracy graph\n",
        "  plt.plot(history.history['zero_one_accuracy'])\n",
        "  plt.plot(history.history['val_zero_one_accuracy'])\n",
        "  plt.title('model zero one accuracy')\n",
        "  plt.ylabel('accuracy')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['train', 'validation'], loc='upper left')\n",
        "  plt.show()\n",
        "  max_result = np.max(history.history['val_zero_one_accuracy'])\n",
        "  print(\"Max zo accuracy on validation: \", max_result)\n",
        "  print()\n",
        "  return max_result #return max validation result\n",
        "\n",
        "\n",
        "\n",
        "def train_and_validate(model_num):\n",
        "  '''train and validate the model, and saves the result in the MODELS array'''\n",
        "  # create model\n",
        "  model = build_model()\n",
        "  print(model.summary())\n",
        "  # create checkpoint for saving best model\n",
        "  filepath=\"model_num=\" + str(model_num) + \",epoch={epoch:03d}.hdf5\"\n",
        "  checkpoint = ModelCheckpoint(filepath, monitor='val_zero_one_accuracy', verbose=1, mode='max', save_weights_only=True,\n",
        "    save_freq=BATCHES_PER_EPOCH*5)#save_best_only=True)#save best model to the file\n",
        "  # es = EarlyStopping(monitor='val_zero_one_accuracy', mode='max', verbose=1, baseline=0.62, patience=0)\n",
        "  filepath=\"model_num=\" + str(model_num) + \",epoch={epoch:03d}(sgd).hdf5\"\n",
        "  sgd_checkpoint = ModelCheckpoint(filepath, monitor='val_zero_one_accuracy', verbose=1, mode='max', save_weights_only=True,\n",
        "    save_freq=BATCHES_PER_EPOCH*5)#save the model's weights in a file\n",
        "  callbacks_list = [checkpoint]\n",
        "  # fit the model using adam and then sgd\n",
        "  adam_history = model.fit(train_samples, train_labels, validation_split=VALIDATION_SPLIT, epochs=EPOCHS, batch_size=BATCH_SIZE, callbacks=callbacks_list, verbose=VERBOSE, shuffle=False)\n",
        "  #changes optimizer to sgd with small learning rate\n",
        "  model.compile(loss='mse', optimizer=SGD(learning_rate=0.00001), metrics=['mse', 'mae', zero_one_accuracy])\n",
        "  callbacks_list = [sgd_checkpoint]\n",
        "  sgd_history = model.fit(train_samples, train_labels, validation_split=VALIDATION_SPLIT, epochs=SGD_EPOCHS, batch_size=BATCH_SIZE, callbacks=callbacks_list, verbose=VERBOSE, shuffle=False)\n",
        "  max_result = print_history(adam_history)\n",
        "  sgd_max_result = print_history(sgd_history)\n",
        "  max_result = max(sgd_max_result, max_result)\n",
        "  model_string = get_model_string()\n",
        "  # save the models in list of dictionaries\n",
        "  keys = ['model','max_result', 'adam_history', 'model_string']\n",
        "  model_dictionary = {}\n",
        "  for key in keys:\n",
        "    model_dictionary[key] = eval(key)\n",
        "  MODELS.append(model_dictionary)   \n",
        "    \n",
        "\n",
        "def grid_search(cnn):\n",
        "  '''grid search on the model with some parameters, gets the value of the CNN variable'''\n",
        "  global BATCH_NORMALIZATION, DROPOUT,DENSE1, DENSE2, DENSE3, DENSE4, LOSS_FN, OPTIMIZER, EPOCHS, CNN, FILTERS1, FILTERS2, CNN_DENSE1, REGULARIZATION_CONST\n",
        "  CNN = cnn\n",
        "  EPOCHS = 150\n",
        "  model_num = 0\n",
        "  get_dataset()\n",
        "  for rc in [0,1e-6,5e-6,1e-5,4e-5,7e-5,1e-4]:\n",
        "    REGULARIZATION_CONST = rc\n",
        "    for optimizer in ['adam']:\n",
        "      OPTIMIZER = optimizer\n",
        "      if CNN:\n",
        "        for filters1 in [5]:\n",
        "          FILTERS1 = filters1\n",
        "          for filters2 in [None]:\n",
        "            FILTERS2 = filters2\n",
        "            for cnn_dense1 in [200]:\n",
        "              CNN_DENSE1 = cnn_dense1\n",
        "              train_and_validate(model_num)\n",
        "              model_num += 1\n",
        "              print(get_model_string())\n",
        "      else:\n",
        "        for width in [25,50,100,150,200,250]:\n",
        "          DENSE1 = DENSE2 = DENSE3 = DENSE4 = width\n",
        "          train_and_validate(model_num)\n",
        "          model_num += 1\n",
        "          print(get_model_string())\n",
        "\n",
        "def check_model():\n",
        "  '''Check the model with the current global variables''' \n",
        "  get_dataset()\n",
        "  print(get_model_string())#print the model's variables\n",
        "  train_and_validate(0)\n",
        "  print(get_model_string())\n",
        "\n",
        "def main():\n",
        "  check_model()\n",
        "  # grid_search(cnn = False)\n",
        "  pass\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  main()\n",
        "  output.eval_js('new Audio(\"https://upload.wikimedia.org/wikipedia/commons/0/05/Beep-09.ogg\").play()') #beeps when finished to run\n",
        "\n",
        "########################################################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hp_-q0b4OdIA"
      },
      "source": [
        "##########Check the final model############\n",
        "\n",
        "#the final model paramters\n",
        "NUM_OF_GAMES_BACKWARD = 8\n",
        "NUM_OF_FEATURES_PER_GAME = 8\n",
        "TRAINING_FILE_NAME = 'team_training_' + str(NUM_OF_GAMES_BACKWARD) + '_' + str(NUM_OF_FEATURES_PER_GAME) + '_25_2004.csv'\n",
        "TEST_FILE_NAME = 'team_test_' + str(NUM_OF_GAMES_BACKWARD) + '_' + str(NUM_OF_FEATURES_PER_GAME) + '_25_2004.csv'\n",
        "DENSE1 = 75\n",
        "DENSE2 = 75\n",
        "DENSE3 = 75\n",
        "DENSE4 = 75\n",
        "\n",
        "def build_model():\n",
        "  '''Builds the final model'''\n",
        "  model = Sequential()\n",
        "  model.add(Dense(DENSE1, input_dim=FEATURES_LEN, activation='relu'))\n",
        "  model.add(Dense(DENSE2, activation='relu'))\n",
        "  model.add(Dense(DENSE3, activation='relu'))\n",
        "  model.add(Dense(DENSE4, activation='relu'))\n",
        "  model.add(Dense(1, activation='sigmoid'))\n",
        "  return model\n",
        "\n",
        "def plot_density(x, data_name):\n",
        "  '''Plots the density of x'''\n",
        "  num_bins = 20\n",
        "  n, bins, patches = plt.hist(x, num_bins, \n",
        "                            density = 1, \n",
        "                            range = (0,1),\n",
        "                            color ='green',\n",
        "                            alpha = 0.7)\n",
        "  \n",
        "  plt.xlabel('Label')\n",
        "  plt.ylabel('Density')\n",
        "  \n",
        "  plt.title('Density hist of the {}\\n'.format(data_name),\n",
        "          fontweight =\"bold\")\n",
        "  \n",
        "  plt.show()\n",
        "\n",
        "def main():\n",
        "  '''Test the final model'''\n",
        "  #get test data\n",
        "  get_dataset()\n",
        "  model = build_model()\n",
        "  filepath = \"drive/MyDrive/model_weights.hdf5\"\n",
        "  model.load_weights(filepath)\n",
        "  test_acc = eval_model(model, test_samples, test_labels)#evaluate the model\n",
        "  print(test_acc)\n",
        "  for down, up in zip([0, 0.08, 0.12],[0.08, 0.12, 0.5]):#calculate certainty for some thresholds\n",
        "    certainty_calculator(model, test_samples, test_labels, down_threshold=down, up_threshold=up)\n",
        "  \n",
        "  # plot_density(model.predict(test_samples), \"test samples\")\n",
        "  # plot_density(train_labels, \"train labels\")\n",
        "  \n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  main()\n",
        "\n",
        "##################################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qpIDZ2_jOgha"
      },
      "source": [
        "##########Trying some ML techniques##########\n",
        "\n",
        "# TRAINING_FILE_NAME = 'player_training_3_8_25.csv'\n",
        "# TEST_FILE_NAME = 'player_test_3_8_25.csv'\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn import svm\n",
        "from sklearn import linear_model\n",
        "\n",
        "get_dataset()\n",
        "\n",
        "print(TRAINING_FILE_NAME)\n",
        "\n",
        "# Create linear regression object\n",
        "regr = linear_model.LinearRegression()\n",
        "\n",
        "# Train the model using the training sets\n",
        "regr.fit(train_samples, train_labels)\n",
        "\n",
        "# Make predictions using the testing set\n",
        "predictions = regr.predict(test_samples)\n",
        "lr_acc = calc_accuracy(predictions, test_labels)\n",
        "\n",
        "print(\"LINEAR REGRESSION GOT ACCURACY OF {0}\".format(lr_acc))\n",
        "\n",
        "\n",
        "change_labels_to_binary()#change the labels to 0 and 1 for svm and decision tree\n",
        "\n",
        "\n",
        "#Create a svm Classifier\n",
        "for kernel in ('linear', 'poly', 'rbf'):\n",
        "\n",
        "  clf = svm.SVC(kernel=kernel) # Linear Kernel\n",
        "\n",
        "  #Train the model using the training sets\n",
        "  clf.fit(train_samples, train_labels)\n",
        "\n",
        "  #Predict the response for test dataset\n",
        "  predictions = clf.predict(test_samples)\n",
        "  svm_acc = calc_accuracy(predictions, test_labels)\n",
        "\n",
        "  print(\"SVM GOT ACCURACY OF {0} WITH {1} KERNEL\".format(svm_acc, kernel.upper()))\n",
        "\n",
        "# Create Decision Tree classifer object\n",
        "clf = DecisionTreeClassifier()\n",
        "\n",
        "\n",
        "# Train Decision Tree Classifer\n",
        "clf = clf.fit(train_samples,train_labels)\n",
        "\n",
        "#Predict the response for test dataset\n",
        "predictions = clf.predict(test_samples)\n",
        "tree_acc = calc_accuracy(predictions, test_labels)\n",
        "\n",
        "print(\"DESICION TREE GOT ACCURACY OF {0}\".format(tree_acc))\n",
        "############################################"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}